# utils/text_processor.py
import re
import string
import unicodedata
from typing import List, Optional
import nltk
from nltk.corpus import stopwords
from nltk.stem import RSLPStemmer
import spacy
from collections import Counter
import os

class TextPreprocessor:
    def __init__(self, language='portuguese'):
        """Inicializa pr√©-processador para portugu√™s"""
        self.language = language
        
        # Verificar e baixar recursos NLTK se necess√°rio
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            print("üì• Baixando recursos NLTK...")
            nltk.download('stopwords', quiet=True)
            nltk.download('rslp', quiet=True)
            nltk.download('punkt', quiet=True)
        
        # Inicializar recursos
        self.stop_words = set(stopwords.words('portuguese'))
        self.stemmer = RSLPStemmer()
        
        # Adicionar stop words espec√≠ficas de email
        email_stopwords = {
            'att', 'attach', 'attachment', 'anexo', 'anexado', 'encaminhado', 
            'forwarded', 're:', 'fw:', 'de:', 'para:', 'assunto:', 'subject:',
            'from:', 'to:', 'date:', 'data:', 'enviado', 'sent', 'message',
            'mensagem', 'email', 'e-mail', 'dear', 'caro', 'prezado', 'cordiais',
            'atenciosamente', 'sinceramente', 'grato', 'obrigado', 'obrigada',
            'cumprimentos', 'sauda√ß√µes', 'regards', 'best', 'thanks', 'thank'
        }
        self.stop_words.update(email_stopwords)
        
        # Carregar spaCy para lematiza√ß√£o (se dispon√≠vel)
        self.nlp = None
        try:
            import spacy
            # Tentar carregar modelo portugu√™s
            try:
                self.nlp = spacy.load("pt_core_news_sm")
                print("‚úÖ spaCy para portugu√™s carregado")
            except:
                # Se modelo portugu√™s n√£o dispon√≠vel, usar ingl√™s e desabilitar lematiza√ß√£o
                print("‚ö†Ô∏è spaCy portugu√™s n√£o dispon√≠vel. Usando stemming.")
                self.nlp = None
        except ImportError:
            print("‚ÑπÔ∏è spaCy n√£o instalado. Usando NLTK para pr√©-processamento.")
            self.nlp = None
    
    def clean_text(self, text: str, remove_html: bool = True) -> str:
        """Limpeza completa do texto"""
        if not text:
            return ""
        
        # Converter para string se necess√°rio
        text = str(text)
        
        # Remover HTML tags
        if remove_html:
            text = re.sub(r'<[^>]+>', '', text)
        
        # Remover URLs
        text = re.sub(r'https?://\S+|www\.\S+', '[URL]', text)
        
        # Remover endere√ßos de email
        text = re.sub(r'\S+@\S+', '[EMAIL]', text)
        
        # Remover n√∫meros de telefone
        text = re.sub(r'\(?\d{2,3}\)?[\s-]?\d{4,5}[\s-]?\d{4}', '[TELEFONE]', text)
        
        # Remover caracteres especiais mas manter pontua√ß√£o b√°sica
        text = re.sub(r'[^\w\s.,!?;:()\-\[\]{}"\']', ' ', text)
        
        # Normalizar espa√ßos
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def normalize_text(self, text: str) -> str:
        """Normaliza√ß√£o do texto (acentos, caixa, etc.)"""
        # Remover acentos
        text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')
        
        # Converter para min√∫sculas
        text = text.lower()
        
        # Remover n√∫meros isolados (mas manter n√∫meros em palavras)
        text = re.sub(r'\b\d+\b', '', text)
        
        return text
    
    def tokenize(self, text: str, remove_stopwords: bool = True) -> List[str]:
        """Tokeniza o texto"""
        # Limpar e normalizar primeiro
        text = self.clean_text(text)
        text = self.normalize_text(text)
        
        # Tokeniza√ß√£o simples (pode ser melhorada com nltk.word_tokenize)
        tokens = text.split()
        
        if remove_stopwords:
            tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]
        
        return tokens
    
    def stem_tokens(self, tokens: List[str]) -> List[str]:
        """Aplica stemming aos tokens"""
        return [self.stemmer.stem(token) for token in tokens if token]
    
    def lemmatize_text(self, text: str) -> str:
        """Aplica lematiza√ß√£o usando spaCy se dispon√≠vel"""
        if self.nlp:
            doc = self.nlp(text)
            lemmas = [token.lemma_ for token in doc if not token.is_stop and len(token.text) > 2]
            return " ".join(lemmas)
        else:
            # Fallback para stemming
            tokens = self.tokenize(text, remove_stopwords=True)
            stemmed = self.stem_tokens(tokens)
            return " ".join(stemmed)
    
    def extract_keywords(self, text: str, top_n: int = 10) -> List[str]:
        """Extrai palavras-chave mais frequentes"""
        tokens = self.tokenize(text, remove_stopwords=True)
        
        # Filtrar tokens por comprimento e conte√∫do
        filtered_tokens = [
            token for token in tokens 
            if len(token) > 3 and token.isalpha()
        ]
        
        # Contar frequ√™ncia
        freq_dist = Counter(filtered_tokens)
        
        # Retornar as top_n mais frequentes
        return [word for word, _ in freq_dist.most_common(top_n)]
    
    def preprocess_for_classification(self, text: str) -> str:
        """Pr√©-processamento otimizado para classifica√ß√£o"""
        # Pipeline completo
        cleaned = self.clean_text(text)
        normalized = self.normalize_text(cleaned)
        lemmatized = self.lemmatize_text(normalized)
        
        return lemmatized
    
    def preprocess_for_summarization(self, text: str) -> str:
        """Pr√©-processamento otimizado para sumariza√ß√£o"""
        # Manter mais estrutura para sumariza√ß√£o
        cleaned = self.clean_text(text, remove_html=True)
        normalized = self.normalize_text(cleaned)
        
        # N√£o lematizar/stemming para sumariza√ß√£o (preserva significado)
        return normalized
    
    def get_email_metadata(self, text: str) -> dict:
        """Extrai metadados √∫teis de emails"""
        metadata = {
            'has_attachments': False,
            'has_links': False,
            'has_dates': False,
            'has_numbers': False,
            'word_count': 0,
            'sentence_count': 0
        }
        
        # Verificar anexos
        attachment_keywords = ['anexo', 'attachment', 'attach', 'encaminhado', 'forward']
        if any(keyword in text.lower() for keyword in attachment_keywords):
            metadata['has_attachments'] = True
        
        # Verificar links
        if re.search(r'https?://|www\.', text):
            metadata['has_links'] = True
        
        # Verificar datas
        date_patterns = [
            r'\d{1,2}/\d{1,2}/\d{2,4}',
            r'\d{1,2}-\d{1,2}-\d{2,4}',
            r'\d{1,2} de [a-z]+ de \d{4}'
        ]
        for pattern in date_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                metadata['has_dates'] = True
                break
        
        # Verificar n√∫meros
        if re.search(r'\b\d+\b', text):
            metadata['has_numbers'] = True
        
        # Contar palavras e senten√ßas
        words = self.tokenize(text, remove_stopwords=False)
        metadata['word_count'] = len(words)
        
        sentences = re.split(r'[.!?]+', text)
        metadata['sentence_count'] = len([s for s in sentences if s.strip()])
        
        return metadata